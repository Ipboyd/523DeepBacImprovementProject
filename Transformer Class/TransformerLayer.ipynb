{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTSjywqZMkZd"
   },
   "source": [
    "Transformer (Multi-head Attention Class)\n",
    "EC523 Project\n",
    "Team 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cqScFtetMhFy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tipg3lG2SEAt"
   },
   "source": [
    "Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s668EQv6ROZJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, num_heads, output_dim):\n",
    "    super(Transformer, self).__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.output_dim = output_dim\n",
    "\n",
    "    self.multihead_attn = nn.MultiheadAttention(embed_dim=self.input_dim, num_heads=self.num_heads, batch_first=True)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    attn_output, attn_weights = self.multihead_attn(x,x,x)\n",
    "\n",
    "    return attn_output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yxvc3XcZSb_u"
   },
   "source": [
    "Generate Random X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPMySKkOSbP3",
    "outputId": "90d7ad9f-4a52-49ea-af93-7a3d40b78b01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Flattened Image: (30, 100)\n"
     ]
    }
   ],
   "source": [
    "Width, Height = 10, 10\n",
    "Images = 30\n",
    "\n",
    "X = np.random.rand(Images, Width, Height).reshape(-1, Width * Height)\n",
    "Y = np.random.rand(Images, Width, Height).reshape(-1, Width * Height)\n",
    "print(f'Shape of Flattened Image:', X.shape)\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 128, 128)\n",
      "(112, 16384)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#The following code loads in the brightfield for the training images in 2b from the storage on the SCC and stores them in a 3D array.\n",
    "#Warning! This code only works for images that are 256 by 256 right now. It is hardcoded\n",
    "\n",
    "#/\\/\\/\\/\\/\\/\\/\\\\/\\/\\/\n",
    "#Training BrightField\n",
    "#/\\/\\/\\/\\/\\/\\/\\\\/\\/\\/\n",
    "\n",
    "# Define the folder containing the TIFF files\n",
    "folder_path = '../../../projectnb/ec523kb/projects/teams_Fall_2024/Team_2/bacteria_counting/Data/2b/DeepBacs_Data_Segmentation_Staph_Aureus_dataset/brightfield_dataset/train/patches/brightfield'\n",
    "\n",
    "# List all TIFF files in the folder\n",
    "tiff_files = [f for f in os.listdir(folder_path) if f.endswith('.tif') or f.endswith('.tiff')]\n",
    "\n",
    "# Initialize a list to store images\n",
    "images = []\n",
    "\n",
    "# Loop through and load each TIFF file\n",
    "for file_name in tiff_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Load the image and convert to a NumPy array\n",
    "    image = np.array(Image.open(file_path))\n",
    "\n",
    "    # Ensure the image is 256x256, or resize if necessary\n",
    "    if image.shape != (256, 256):\n",
    "        image = np.array(Image.open(file_path).resize((256, 256)))\n",
    "\n",
    "    # Append the image to the list\n",
    "    # images.append(image)\n",
    "    \n",
    "    #plt.imshow(image)  # Use cmap='gray' for grayscale images\n",
    "    #plt.axis('off')  # Turn off the axis\n",
    "    #plt.show()\n",
    "    \n",
    "    # Split the image into 4 quarters\n",
    "    top_left = image[:128, :128]  # Top-left quarter\n",
    "    top_right = image[:128, 128:]  # Top-right quarter\n",
    "    bottom_left = image[128:, :128]  # Bottom-left quarter\n",
    "    bottom_right = image[128:, 128:]  # Bottom-right quarter\n",
    "\n",
    "    # Stack the quarters to form a (4, 64, 64) array\n",
    "    quarters_image = np.stack([top_left, top_right, bottom_left, bottom_right])\n",
    "\n",
    "    # Append the quarters image to the list\n",
    "    images.append(quarters_image)\n",
    "    \n",
    "\n",
    "# Convert the list of images into a 3D NumPy array\n",
    "image_stack = np.stack(images)\n",
    "\n",
    "image_stack = image_stack.reshape(-1, 128, 128)\n",
    "\n",
    "#print(image_stack.shape)  # Should be (n_images, 256, 256)\n",
    "\n",
    "\n",
    "#/\\/\\/\\/\\/\\/\\/\\\\/\\/\\/\n",
    "#Training masks\n",
    "#/\\/\\/\\/\\/\\/\\/\\\\/\\/\\/\n",
    "\n",
    "# Define the folder containing the TIFF files\n",
    "folder_path = '../../../projectnb/ec523kb/projects/teams_Fall_2024/Team_2/bacteria_counting/Data/2b/DeepBacs_Data_Segmentation_Staph_Aureus_dataset/brightfield_dataset/train/patches/masks'\n",
    "\n",
    "# List all TIFF files in the folder\n",
    "tiff_files = [f for f in os.listdir(folder_path) if f.endswith('.tif') or f.endswith('.tiff')]\n",
    "\n",
    "# Initialize a list to store images\n",
    "images = []\n",
    "\n",
    "# Loop through and load each TIFF file\n",
    "for file_name in tiff_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Load the image and convert to a NumPy array\n",
    "    image = np.array(Image.open(file_path))\n",
    "\n",
    "    # Ensure the image is 256x256, or resize if necessary\n",
    "    if image.shape != (256, 256):\n",
    "        image = np.array(Image.open(file_path).resize((256, 256)))\n",
    "\n",
    "    # Append the image to the list\n",
    "    # images.append(image)\n",
    "    \n",
    "    #plt.imshow(image)  # Use cmap='gray' for grayscale images\n",
    "    #plt.axis('off')  # Turn off the axis\n",
    "    #plt.show()\n",
    "    \n",
    "    # Split the image into 4 quarters\n",
    "    top_left = image[:128, :128]  # Top-left quarter\n",
    "    top_right = image[:128, 128:]  # Top-right quarter\n",
    "    bottom_left = image[128:, :128]  # Bottom-left quarter\n",
    "    bottom_right = image[128:, 128:]  # Bottom-right quarter\n",
    "\n",
    "    # Stack the quarters to form a (4, 64, 64) array\n",
    "    quarters_image = np.stack([top_left, top_right, bottom_left, bottom_right])\n",
    "\n",
    "    # Append the quarters image to the list\n",
    "    images.append(quarters_image)\n",
    "\n",
    "# Convert the list of images into a 3D NumPy array\n",
    "image_stack_masks = np.stack(images)\n",
    "\n",
    "image_stack_masks = image_stack_masks.reshape(-1, 128, 128)\n",
    "\n",
    "\n",
    "#print(image_stack_masks.shape)  # Should be (n_images, 256, 256)\n",
    "\n",
    "#X = np.random.rand(Images, Width, Height).reshape(-1, Width * Height)\n",
    "#Y = np.random.rand(Images, Width, Height).reshape(-1, Width * Height)\n",
    "#print(f'Shape of Flattened Image:', X.shape)\n",
    "\n",
    "print(image_stack.shape)\n",
    "\n",
    "image_stack = image_stack.reshape(-1, image_stack.shape[1] * image_stack.shape[2])\n",
    "image_stack_masks = image_stack_masks.reshape(-1, image_stack_masks.shape[1] * image_stack_masks.shape[2])\n",
    "\n",
    "print(image_stack.shape)\n",
    "\n",
    "X_tensor = torch.tensor(image_stack, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(image_stack_masks, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnkD15L4TPnn"
   },
   "source": [
    "Instaniate Class (Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bfpb738vTUdp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate Model Class\n",
    "transformer_model = Transformer(input_dim=16384, hidden_dim=10, num_heads=4, output_dim=16384)\n",
    "\n",
    "# Set Optimizer and Training Loss\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58a-8ToIWfeu",
    "outputId": "8bec10ac-16fb-4ff2-80f2-78df71dfa0ba",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: multihead_attn.in_proj_weight\n",
      "Shape: torch.Size([49152, 16384])\n",
      "Parameter name: multihead_attn.in_proj_bias\n",
      "Shape: torch.Size([49152])\n",
      "Parameter name: multihead_attn.out_proj.weight\n",
      "Shape: torch.Size([16384, 16384])\n",
      "Parameter name: multihead_attn.out_proj.bias\n",
      "Shape: torch.Size([16384])\n"
     ]
    }
   ],
   "source": [
    "# Print Model Parameters\n",
    "for name, param in transformer_model.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbZPFMOyXKNE"
   },
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dc0rLGV2YNhR",
    "outputId": "754ff7ee-5d2d-43fa-f57c-c6ed4de4ac1b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (multihead_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=16384, out_features=16384, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbk0DnP6XJvl",
    "outputId": "a8a8256e-c504-4bea-d406-8243ecd721a2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr4/ec523/ipboyd/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([2, 16384])) that is different to the input size (torch.Size([2, 1, 16384])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 176366627515.1250\n",
      "Epoch [2/100], Loss: 109546750.0134\n",
      "Epoch [3/100], Loss: 10873376.6295\n",
      "Epoch [4/100], Loss: 9834765.7720\n",
      "Epoch [5/100], Loss: 9904183.6116\n",
      "Epoch [6/100], Loss: 6998101.3742\n",
      "Epoch [7/100], Loss: 6934522.3583\n",
      "Epoch [8/100], Loss: 4873388.0056\n",
      "Epoch [9/100], Loss: 3698774.4464\n",
      "Epoch [10/100], Loss: 3393951.6722\n",
      "Epoch [11/100], Loss: 2993138.4752\n",
      "Epoch [12/100], Loss: 2666033.9104\n",
      "Epoch [13/100], Loss: 1910382.5156\n",
      "Epoch [14/100], Loss: 1786948.7335\n",
      "Epoch [15/100], Loss: 1492270.9060\n",
      "Epoch [16/100], Loss: 1280306.1719\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    transformer_model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.view(inputs.size(0), -1, 16384)  # Reshape for input_dim=100\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = transformer_model(inputs)\n",
    "        loss_value = loss(outputs, targets)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss_value.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "180fW2wJY73p"
   },
   "source": [
    "Model Testing"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
