{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d70cb-e8a3-4e86-9546-876d2e65214e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3052a828-d2a4-4075-80cd-e6b29f445e1f",
   "metadata": {},
   "source": [
    "# Integrating CNN to Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72421601-6d68-4d17-8a7d-e6b0396c598e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_images(folder_path, threshold=None):\n",
    "    \"\"\"\n",
    "    Load TIFF images and optional thresholding for masks without downsampling.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing TIFF images.\n",
    "        threshold (int, optional): Threshold value for binary masks. Default is None.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Array of original images.\n",
    "    \"\"\"\n",
    "    tiff_files = [f for f in os.listdir(folder_path) if f.endswith('.tif') or f.endswith('.tiff')]\n",
    "    images = []  # Store original images\n",
    "\n",
    "    for file_name in tiff_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        image = Image.open(file_path)  # Load the image\n",
    "        image_array = np.array(image)  # Convert to NumPy array\n",
    "\n",
    "        # Apply threshold for binary masks (if provided)\n",
    "        if threshold is not None:\n",
    "            image_array = (image_array > threshold).astype(np.uint8)\n",
    "\n",
    "        images.append(image_array)  # Append to the list\n",
    "\n",
    "    return np.stack(images)  # Return as a single NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce060fa-0a67-4f6d-bee0-10d3ed73f1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "brightfield_folder = '/projectnb/ec523kb/projects/teams_Fall_2024/Team_2/bacteria_counting/Data/2b/DeepBacs_Data_Segmentation_Staph_Aureus_dataset/brightfield_dataset/train/patches/brightfield'\n",
    "masks_folder = '/projectnb/ec523kb/projects/teams_Fall_2024/Team_2/bacteria_counting/Data/2b/DeepBacs_Data_Segmentation_Staph_Aureus_dataset/brightfield_dataset/train/patches/masks'\n",
    "\n",
    "# Load brightfield images and masks\n",
    "original_brightfield = load_images(brightfield_folder)\n",
    "original_masks = load_images(masks_folder, threshold=1)\n",
    "\n",
    "# Convert images and masks to PyTorch tensors\n",
    "X_tensor = torch.tensor(original_brightfield, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "Y_tensor = torch.tensor(original_masks, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)  # Pairs of images and masks\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)  # Modify batch size if needed\n",
    "\n",
    "#TODO: We need to add validation data!\n",
    "\n",
    "# Check shapes\n",
    "print(f\"Original Brightfield Tensor Shape: {X_tensor.shape}\")  # Expected: (batch_size, 1, H, W)\n",
    "print(f\"Original Masks Tensor Shape: {Y_tensor.shape}\")        # Expected: (batch_size, 1, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d8e0d-a6f2-4cd1-8298-7e460f64fa78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_examples = 5  # Number of examples to display\n",
    "plt.figure(figsize=(10, num_examples * 3))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    plt.subplot(num_examples, 2, i * 2 + 1)\n",
    "    plt.imshow(original_brightfield[i], cmap=\"gray\")\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(num_examples, 2, i * 2 + 2)\n",
    "    plt.imshow(original_masks[i], cmap=\"gray\")\n",
    "    plt.title(\"Original Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad381672-fcb9-453e-bc7c-c73915380745",
   "metadata": {},
   "source": [
    "## CNN Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c647dd-04af-428f-addd-d32666398657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, output_channels=64):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # First Conv Layer\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Downsample by 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Second Conv Layer\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Downsample by 2\n",
    "            nn.Conv2d(64, output_channels, kernel_size=3, stride=1, padding=1),  # Final Conv Layer\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf2abbf-8c79-4ef2-b715-6ff384a161bc",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4daf5f-4446-47b5-aad2-123f316baaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, embed_dim, num_heads, num_layers, cnn_extractor):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.cnn_extractor = cnn_extractor\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.position_embedding = None\n",
    "        self.transformer = nn.Transformer(embed_dim, num_heads, num_layers, num_layers)\n",
    "        self.head = nn.Linear(embed_dim, patch_size * patch_size)\n",
    "\n",
    "        # Upsampling layer to match target size\n",
    "        self.upsample = nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_extractor(x)  # (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        #print(f\"[DEBUG] CNN Output Shape: {x.shape}\")\n",
    "\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')  # (B, num_patches, embed_dim)\n",
    "        num_patches = x.size(1)\n",
    "\n",
    "        if self.position_embedding is None or self.position_embedding.size(1) != num_patches:\n",
    "            self.position_embedding = nn.Parameter(torch.randn(1, num_patches, self.embed_dim).to(x.device))\n",
    "\n",
    "        x = x + self.position_embedding\n",
    "        x = self.transformer(x, x)\n",
    "        #print(f\"[DEBUG] Transformer Output Shape: {x.shape}\")\n",
    "\n",
    "        h_patches = w_patches = int(num_patches ** 0.5)\n",
    "        x = self.head(x)\n",
    "        x = rearrange(x, 'b (h_patches w_patches) (p1 p2) -> b 1 (h_patches p1) (w_patches p2)',\n",
    "                      h_patches=h_patches, w_patches=w_patches, p1=self.patch_size, p2=self.patch_size)\n",
    "        #print(f\"[DEBUG] Rearranged Output Shape: {x.shape}\")\n",
    "\n",
    "        # Upsample to match target size\n",
    "        x = self.upsample(x)\n",
    "        #print(f\"[DEBUG] Final Upsampled Output Shape: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d453b-471f-437e-aa37-2f4f6d4099b1",
   "metadata": {},
   "source": [
    "## Visualization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3abeb6-e6f2-4165-9cd9-c6ad72d0286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize predictions\n",
    "def visualize_predictions(images, masks, outputs, num_examples=4):\n",
    "    \"\"\"\n",
    "    Visualize a few examples of the input images, ground truth masks, and model predictions.\n",
    "\n",
    "    Args:\n",
    "        images (torch.Tensor): Input images.\n",
    "        masks (torch.Tensor): Ground truth masks.\n",
    "        outputs (torch.Tensor): Model predictions.\n",
    "        num_examples (int): Number of examples to visualize.\n",
    "    \"\"\"\n",
    "    # Convert tensors to CPU for visualization\n",
    "    images = images[:num_examples].cpu().numpy()\n",
    "    masks = masks[:num_examples].cpu().numpy()\n",
    "    outputs = torch.sigmoid(outputs[:num_examples]).cpu().numpy()  # Apply sigmoid to get probabilities\n",
    "\n",
    "    # Plot the results\n",
    "    fig, axes = plt.subplots(num_examples, 3, figsize=(12, num_examples * 4))\n",
    "    for i in range(num_examples):\n",
    "        # Input image\n",
    "        axes[i, 0].imshow(images[i, 0], cmap=\"gray\")\n",
    "        axes[i, 0].set_title(\"Input Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        # Ground truth mask\n",
    "        axes[i, 1].imshow(masks[i, 0], cmap=\"gray\")\n",
    "        axes[i, 1].set_title(\"Ground Truth Mask\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        # Model prediction\n",
    "        axes[i, 2].imshow(outputs[i, 0], cmap=\"gray\")\n",
    "        axes[i, 2].set_title(\"Model Prediction\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f03ae-a99a-4252-bec8-d700a8767f95",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88cfdb9-fb8e-42b1-b9d1-328c9bb6ef13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialize the CNN\n",
    "cnn_extractor = CNNFeatureExtractor(output_channels=64)\n",
    "\n",
    "# Initialize the Transformer with CNN as backbone\n",
    "model = SimpleTransformer(img_size=64, patch_size=8, embed_dim=64, num_heads=4, num_layers=4, cnn_extractor=cnn_extractor).to(device)\n",
    "\n",
    "# Train the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "num_epochs = 300\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for images, masks in dataloader:  # Assuming dataloader includes all data\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Calculate and print average loss for the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Visualize predictions after each epoch (optional)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            visualize_predictions(images, masks, outputs, num_examples=2) # Change num examples if more visuals are wanted\n",
    "            break  # Visualize only the first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf5966-d399-4ea7-9327-0329b8e25814",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8d109-2df5-4637-873b-a4532d26b0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the first batch of the last epoch and loss of last epoch\n",
    "print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in dataloader:  \n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        visualize_predictions(images, masks, outputs, num_examples=4)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae9ad5-8a48-4dc0-8dd0-373c8e5c6c21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"cnn_transformer_model.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
