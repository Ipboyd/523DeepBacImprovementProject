{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0edfdc-906c-40f1-9b88-950dd1f90216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from skimage.draw import disk\n",
    "from skimage.measure import label, regionprops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Flatten tensors\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice_score = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "        return 1 - dice_score\n",
    "    \n",
    "def false_positive_loss(predicted_masks, ground_truth_masks):\n",
    "    # Focus on ground truth masks that are empty\n",
    "    empty_mask = (ground_truth_masks.sum(dim=(1, 2, 3)) == 0).float()  # Batch size dimension\n",
    "    false_positive = (predicted_masks * empty_mask.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1))\n",
    "    return false_positive.mean()\n",
    "\n",
    "def extra_region_penalty(predicted_masks, ground_truth_masks):\n",
    "    non_overlap = predicted_masks * (1 - ground_truth_masks)  # Predicted regions outside GT\n",
    "    return non_overlap.mean()\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, base_loss, alpha=1.0, beta=1.0):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.alpha = alpha  # Weight for false positive penalty\n",
    "        self.beta = beta    # Weight for extra region penalty\n",
    "\n",
    "    def forward(self, predicted_masks, ground_truth_masks):\n",
    "        base_loss = self.base_loss(predicted_masks, ground_truth_masks)\n",
    "        fp_penalty = false_positive_loss(predicted_masks, ground_truth_masks)\n",
    "        extra_penalty = extra_region_penalty(predicted_masks, ground_truth_masks)\n",
    "        return base_loss + self.alpha * fp_penalty + self.beta * extra_penalty\n",
    "\n",
    "class CircleDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, image_size=(64, 64), max_circles=5):\n",
    "        self.binary_masks, self.instance_masks = self.create_dataset(num_samples, image_size, max_circles)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.binary_masks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        binary_mask = self.binary_masks[idx]\n",
    "        instance_mask = self.instance_masks[idx]\n",
    "        instance_mask = np.pad(instance_mask, ((0, 0), (0, 0), (0, 5 - instance_mask.shape[2])), constant_values=0)\n",
    "        return torch.tensor(binary_mask, dtype=torch.float32).unsqueeze(0), torch.tensor(instance_mask, dtype=torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_dataset(num_samples=1000, image_size=(64, 64), max_circles=5):\n",
    "        binary_masks = []\n",
    "        instance_masks = []\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            num_circles = random.randint(1, max_circles)\n",
    "            _, binary_mask, _, _ = generate_image_with_circles(image_size, num_circles)\n",
    "            instances = generate_instance_masks(binary_mask)\n",
    "\n",
    "            binary_masks.append(binary_mask)\n",
    "            instance_stack = np.stack(instances, axis=-1) if instances else np.zeros((*image_size, 0))\n",
    "            instance_masks.append(instance_stack)\n",
    "\n",
    "        return np.array(binary_masks), instance_masks\n",
    "\n",
    "def generate_image_with_circles(image_size=(64, 64), num_circles=5):\n",
    "    \"\"\"Generate an image with random circles and return the image and binary mask.\"\"\"\n",
    "    image = np.zeros(image_size, dtype=np.uint8)\n",
    "    binary_mask = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "    for _ in range(num_circles):\n",
    "        radius = random.randint(4, 8)\n",
    "        center = (\n",
    "            random.randint(radius, image_size[0] - radius),\n",
    "            random.randint(radius, image_size[1] - radius),\n",
    "        )\n",
    "        rr, cc = disk(center, radius, shape=image_size)\n",
    "        binary_mask[rr, cc] = 1\n",
    "\n",
    "    return image, binary_mask, [], []\n",
    "\n",
    "def generate_instance_masks(binary_mask):\n",
    "    \"\"\"Generate instance masks from a binary mask.\"\"\"\n",
    "    labeled_mask = label(binary_mask)\n",
    "    instance_masks = []\n",
    "    for region in regionprops(labeled_mask):\n",
    "        instance_mask = labeled_mask == region.label\n",
    "        instance_masks.append(instance_mask)\n",
    "    return instance_masks\n",
    "\n",
    "def visualize_results(image, binary_mask, instance_masks):\n",
    "    \"\"\"Visualize the image, binary mask, and instance masks.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2 + len(instance_masks), figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(binary_mask, cmap='gray')\n",
    "    axes[1].set_title(\"Binary Mask\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    for i, instance_mask in enumerate(instance_masks):\n",
    "        axes[2 + i].imshow(instance_mask, cmap='gray')\n",
    "        axes[2 + i].set_title(f\"Instance Mask {i + 1}\")\n",
    "        axes[2 + i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class TransformerSegmentationModel(nn.Module):   # 8,64   # 4,256  # 2,1024  # 1,4096\n",
    "    def __init__(self, img_size=64, patch_size=4, num_patches=256, in_channels=1, out_channels=5, embed_dim=256, num_heads=8, num_layers=4):\n",
    "        super(TransformerSegmentationModel, self).__init__()\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.flatten_dim = patch_size * patch_size * in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.embedding = nn.Linear(self.flatten_dim, embed_dim)\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.output_proj = nn.Linear(embed_dim, patch_size * patch_size * out_channels)\n",
    "        \n",
    "        #self.output_proj2 = nn.Linear(img_size,img_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        #print('forwards pass')\n",
    "        #print(x.shape)\n",
    "\n",
    "        # Divide image into patches\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        x = x.view(batch_size, self.num_patches, -1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "\n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(x) + self.position_embedding\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "\n",
    "        # Output projection and reshape\n",
    "        #x = self.output_proj(x)\n",
    "        #x = x.view(batch_size, self.img_size // self.patch_size, self.img_size // self.patch_size, -1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "\n",
    "        \n",
    "        #x = x.view(batch_size,self.img_size,self.out_channels,self.img_size)\n",
    "        \n",
    "        #x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.output_proj(x)  # Shape: (batch_size, num_patches, patch_size * patch_size * out_channels)\n",
    "        \n",
    "        #print(x.shape)\n",
    "\n",
    "        # Reshape into patches\n",
    "        x = x.view(\n",
    "            batch_size,\n",
    "            self.img_size // self.patch_size,  # Number of patches along height\n",
    "            self.img_size // self.patch_size,  # Number of patches along width\n",
    "            self.patch_size,\n",
    "            self.patch_size,\n",
    "            self.out_channels,\n",
    "        )  # Shape: (batch_size, h_patches, w_patches, patch_h, patch_w, out_channels)\n",
    "\n",
    "        # Permute to combine patches into spatial dimensions\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()  # Shape: (batch_size, h_patches * patch_h, w_patches * patch_w, out_channels)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = x.view(batch_size, self.img_size, self.img_size, self.out_channels)  # Shape: (batch_size, img_size, img_size, out_channels)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #print(x.shape)\n",
    "\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class TransformerSegmentationModel(nn.Module):\n",
    "#     def __init__(self, img_size=64, patch_size=4, num_patches=256, in_channels=1, out_channels=3, embed_dim=640, num_heads=8, num_layers=12):\n",
    "#         super(TransformerSegmentationModel, self).__init__()\n",
    "\n",
    "#         self.img_size = img_size\n",
    "#         self.patch_size = patch_size\n",
    "#         self.num_patches = num_patches\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.out_channels = out_channels\n",
    "\n",
    "#         # Replace nn.Linear with a convolutional embedding layer\n",
    "#         self.embedding = nn.Conv2d(\n",
    "#             in_channels=in_channels,\n",
    "#             out_channels=embed_dim,\n",
    "#             kernel_size=patch_size,\n",
    "#             stride=patch_size\n",
    "#         )\n",
    "\n",
    "#         self.position_embedding = nn.Parameter(torch.randn(1, embed_dim, img_size // patch_size, img_size // patch_size))\n",
    "\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads)\n",
    "#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "#         self.output_proj = nn.Conv2d(\n",
    "#             in_channels=embed_dim,\n",
    "#             out_channels=out_channels,\n",
    "#             kernel_size=1\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.size(0)\n",
    "\n",
    "#         # Apply convolutional embedding to input\n",
    "#         x = self.embedding(x)  # Shape: (batch_size, embed_dim, H', W')\n",
    "#         x = x + self.position_embedding  # Add positional encoding\n",
    "\n",
    "#         # Reshape for the Transformer: (batch_size, embed_dim, num_patches) -> (num_patches, batch_size, embed_dim)\n",
    "#         x = x.flatten(2).permute(2, 0, 1)\n",
    "\n",
    "#         # Transformer\n",
    "#         x = self.transformer(x)  # Shape: (num_patches, batch_size, embed_dim)\n",
    "\n",
    "#         # Reshape back: (num_patches, batch_size, embed_dim) -> (batch_size, embed_dim, H', W')\n",
    "#         x = x.permute(1, 2, 0).view(batch_size, self.embed_dim, self.img_size // self.patch_size, self.img_size // self.patch_size)\n",
    "\n",
    "#         # Output projection\n",
    "#         x = self.output_proj(x)  # Shape: (batch_size, out_channels, H, W)\n",
    "\n",
    "#         return torch.sigmoid(x)\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "# Dataset and DataLoader\n",
    "num_samples = 100\n",
    "train_dataset = CircleDataset(num_samples=num_samples)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = TransformerSegmentationModel().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for binary_mask, instance_masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        binary_mask = binary_mask.to(device)  # Move binary_mask to device\n",
    "        instance_masks = instance_masks.to(device) \n",
    "        \n",
    "        outputs = model(binary_mask)\n",
    "        \n",
    "        #print('outputs')\n",
    "        #print(outputs.shape)\n",
    "        #print(instance_masks.shape)\n",
    "        \n",
    "        #bce_loss  = criterion(outputs, instance_masks)\n",
    "        #dice_loss = DiceLoss()(outputs, instance_masks)\n",
    "        #loss = bce_loss + dice_loss\n",
    "        \n",
    "        base_loss = nn.BCELoss()\n",
    "        custom_loss = CustomLoss(base_loss, alpha=2.0, beta=10.0)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = custom_loss(outputs, instance_masks)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            \n",
    "            # Visualizing the binary mask\n",
    "            binary_mask = binary_mask[0].squeeze(0).cpu().numpy()\n",
    "            instance_masks = instance_masks[0].cpu().numpy()\n",
    "            predicted_masks = outputs[0].cpu().numpy()\n",
    "\n",
    "            fig, axes = plt.subplots(2, max(instance_masks.shape[-1], predicted_masks.shape[-1]) + 1, figsize=(18, 6))\n",
    "\n",
    "            # Binary Mask\n",
    "            axes[0, 0].imshow(binary_mask, cmap='gray')\n",
    "            axes[0, 0].set_title(\"Binary Mask\")\n",
    "            axes[0, 0].axis(\"off\")\n",
    "\n",
    "            # Ground Truth Masks\n",
    "            for i in range(instance_masks.shape[-1]):\n",
    "                axes[0, i + 1].imshow(instance_masks[..., i], cmap='gray')\n",
    "                axes[0, i + 1].set_title(f\"GT Mask {i+1}\")\n",
    "                axes[0, i + 1].axis(\"off\")\n",
    "\n",
    "            # Predicted Masks\n",
    "            for i in range(predicted_masks.shape[-1]):\n",
    "                axes[1, i + 1].imshow(predicted_masks[..., i], cmap='gray')\n",
    "                axes[1, i + 1].set_title(f\"Pred Mask {i+1}\")\n",
    "                axes[1, i + 1].axis(\"off\")\n",
    "\n",
    "            # Sum of Predicted Masks\n",
    "            predicted_sum = np.sum(predicted_masks, axis=-1)\n",
    "            axes[1, 0].imshow(predicted_sum, cmap='gray')\n",
    "            axes[1, 0].set_title(\"Sum of Predicted Masks\")\n",
    "            axes[1, 0].axis(\"off\")\n",
    "\n",
    "            # Layout adjustment\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
