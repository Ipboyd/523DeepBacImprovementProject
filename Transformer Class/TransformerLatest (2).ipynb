{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65da97-e51e-4259-ad9b-8751c1bed6da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define a downsampling factor (e.g., reduce to 128x128)\n",
    "downsample_size = (64, 64)\n",
    "\n",
    "# Function to load, store original, and downsample TIFF images\n",
    "def load_and_downsample_images(folder_path, downsample_size, threshold=None):\n",
    "    tiff_files = [f for f in os.listdir(folder_path) if f.endswith('.tif') or f.endswith('.tiff')]\n",
    "    original_images = []  # Store original images\n",
    "    downsampled_images = []  # Store downsampled images\n",
    "\n",
    "    for file_name in tiff_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        image = Image.open(file_path)#.convert(\"L\")  # Convert to grayscale\n",
    "        original_images.append(np.array(image))  # Store the original image\n",
    "\n",
    "\n",
    "    \n",
    "        # Apply threshold if provided (for masks)\n",
    "        if threshold is not None:\n",
    "            #image = np.array(image)\n",
    "            # Use nearest-neighbor interpolation\n",
    "            #downsampled_mask = Image.fromarray(image).resize((downsample_size), Image.NEAREST)\n",
    "            # Resize to the downsample size\n",
    "            resized_image = image.resize(downsample_size)\n",
    "            resized_array = np.array(resized_image)\n",
    "            #sigma = 1.0  # Adjust based on noise level\n",
    "            #smoothed_image = gaussian_filter(image, sigma=sigma)\n",
    "            #downsampled_image = Image.fromarray(smoothed_image).resize(downsample_size)\n",
    "            \n",
    "            \n",
    "            resized_array = (resized_array > threshold).astype(np.uint8)  # Binary mask\n",
    "            downsampled_images.append(resized_array)\n",
    "        else:\n",
    "            \n",
    "            sigma = 3.0  # Adjust based on noise level\n",
    "            smoothed_image = gaussian_filter(image, sigma=sigma)\n",
    "            downsampled_image = Image.fromarray(smoothed_image).resize(downsample_size)\n",
    "            downsampled_images.append(downsampled_image)\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "    return np.stack(original_images), np.stack(downsampled_images)  # Return both original and downsampled\n",
    "\n",
    "# Load original and downsampled brightfield images\n",
    "brightfield_folder = '../../../projectnb/ec523kb/projects/teams_Fall_2024/Team_2/bacteria_counting/Data/2b/DeepBacs_Data_Segmentation_Staph_Aureus_dataset/brightfield_dataset/train/patches/brightfield'\n",
    "original_brightfield, downsampled_brightfield = load_and_downsample_images(brightfield_folder, downsample_size)\n",
    "\n",
    "# Load original and downsampled masks\n",
    "masks_folder = '../../../projectnb/ec523kb/projects/teams_Fall_2024/Team_2/bacteria_counting/Data/2b/DeepBacs_Data_Segmentation_Staph_Aureus_dataset/brightfield_dataset/train/patches/masks'\n",
    "original_masks, downsampled_masks = load_and_downsample_images(masks_folder, downsample_size, threshold=1)\n",
    "\n",
    "# Convert downsampled images and masks to tensors\n",
    "X_tensor = torch.tensor(downsampled_brightfield, dtype=torch.float32).unsqueeze(1)  # (batch_size, 1, H, W)\n",
    "Y_tensor = torch.tensor(downsampled_masks, dtype=torch.float32).unsqueeze(1)  # (batch_size, 1, H, W)\n",
    "\n",
    "# Check the new shapes\n",
    "print(f\"Original Brightfield shape: {original_brightfield.shape}\")  # (batch_size, original_height, original_width)\n",
    "print(f\"Downsampled Brightfield shape: {X_tensor.shape}\")  # (batch_size, 1, downsampled_height, downsampled_width)\n",
    "\n",
    "# Display a few examples\n",
    "num_examples = 20  # Number of examples to show\n",
    "plt.figure(figsize=(12, num_examples * 3))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    # Original brightfield image\n",
    "    plt.subplot(num_examples, 4, i * 4 + 1)\n",
    "    plt.imshow(original_brightfield[i])\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Downsampled brightfield image\n",
    "    plt.subplot(num_examples, 4, i * 4 + 2)\n",
    "    plt.imshow(X_tensor[i, 0].numpy())\n",
    "    plt.title(\"Downsampled Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Original mask\n",
    "    plt.subplot(num_examples, 4, i * 4 + 3)\n",
    "    plt.imshow(original_masks[i])\n",
    "    plt.title(\"Original Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Downsampled mask\n",
    "    plt.subplot(num_examples, 4, i * 4 + 4)\n",
    "    plt.imshow(Y_tensor[i, 0].numpy())\n",
    "    plt.title(\"Downsampled Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48dd50b-f6d7-4011-b3d3-a39fa0f4a27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, embed_dim, num_heads, num_layers):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embedding = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches, embed_dim))\n",
    "        self.transformer = nn.Transformer(embed_dim, num_heads, num_layers, num_layers)\n",
    "        self.head = nn.Linear(embed_dim, patch_size * patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Patchify\n",
    "        x = self.patch_embedding(x)  # (B, embed_dim, H', W')\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')  # (B, num_patches, embed_dim)\n",
    "        x = x + self.position_embedding\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer(x, x)  # (B, num_patches, embed_dim)\n",
    "\n",
    "        # Output head\n",
    "        x = self.head(x)  # (B, num_patches, patch_size * patch_size)\n",
    "        x = rearrange(x, 'b (h w) (p1 p2) -> b 1 (h p1) (w p2)', h=int(self.img_size/self.patch_size), w=int(self.img_size/self.patch_size), p1=self.patch_size, p2=self.patch_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8322c38e-8b08-484f-aab0-f82d54740af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Model and GPU setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleTransformer(img_size=64, patch_size=2, embed_dim=64, num_heads=4, num_layers=4).to(device)\n",
    "#model = SimpleTransformer(img_size=64, patch_size=2, embed_dim=512, num_heads=8, num_layers=8).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)  # Pairs of images and masks\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)  # Modify batch size if needed\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "\n",
    "    model.train()  # Ensure model is in training mode\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        #images = images.unsqueeze(1)  # Add channel dimension\n",
    "        #masks = masks.unsqueeze(1)  # Add channel dimension\n",
    "        #print(masks.shape)\n",
    "\n",
    "        #images = images[:, 0:1, :, :]  # Take the first channel only (if 2 channels exist)\n",
    "        \n",
    "        \n",
    "        \n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Log training loss\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    \n",
    "    # Visualize predictions on the first batch\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            #images = images.unsqueeze(1)  # Add channel dimension\n",
    "            #masks = masks.unsqueeze(1)  # Add channel dimension\n",
    "            outputs = torch.sigmoid(model(images))  # Apply sigmoid to get probabilities\n",
    "            break  # Only visualize the first batch\n",
    "\n",
    "    # Convert tensors to numpy arrays for visualization\n",
    "    images_np = images.cpu().numpy()\n",
    "    masks_np = masks.cpu().numpy()\n",
    "    outputs_np = outputs.cpu().numpy()\n",
    "\n",
    "    # Plot input, ground truth, and predictions\n",
    "    fig, axes = plt.subplots(3, 8, figsize=(12, 9))\n",
    "    for i in range(8):  # Show up to 4 examples from the batch\n",
    "        axes[0, i].imshow(images_np[i, 0], cmap=\"gray\")\n",
    "        axes[0, i].set_title(\"Input\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "\n",
    "        axes[1, i].imshow(masks_np[i, 0], cmap=\"gray\")\n",
    "        axes[1, i].set_title(\"Ground Truth\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "        axes[2, i].imshow(outputs_np[i, 0], cmap=\"gray\")\n",
    "        axes[2, i].set_title(\"Prediction\")\n",
    "        axes[2, i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d70cb-e8a3-4e86-9546-876d2e65214e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install einops"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
