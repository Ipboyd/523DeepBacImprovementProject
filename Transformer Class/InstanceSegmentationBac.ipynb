{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dce5e9-0da3-49a6-b7ae-12f4bc9dcae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from skimage.draw import disk\n",
    "from skimage.measure import label, regionprops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Flatten tensors\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice_score = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "        return 1 - dice_score\n",
    "    \n",
    "def false_positive_loss(predicted_masks, ground_truth_masks):\n",
    "    # Focus on ground truth masks that are empty\n",
    "    empty_mask = (ground_truth_masks.sum(dim=(1, 2, 3)) == 0).float()  # Batch size dimension\n",
    "    false_positive = (predicted_masks * empty_mask.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1))\n",
    "    return false_positive.mean()\n",
    "\n",
    "def extra_region_penalty(predicted_masks, ground_truth_masks):\n",
    "    non_overlap = predicted_masks * (1 - ground_truth_masks)  # Predicted regions outside GT\n",
    "    return non_overlap.mean()\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, base_loss, alpha=1.0, beta=1.0):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.alpha = alpha  # Weight for false positive penalty\n",
    "        self.beta = beta    # Weight for extra region penalty\n",
    "\n",
    "    def forward(self, predicted_masks, ground_truth_masks):\n",
    "        base_loss = self.base_loss(predicted_masks, ground_truth_masks)\n",
    "        fp_penalty = false_positive_loss(predicted_masks, ground_truth_masks)\n",
    "        extra_penalty = extra_region_penalty(predicted_masks, ground_truth_masks)\n",
    "        return base_loss + self.alpha * fp_penalty + self.beta * extra_penalty\n",
    "\n",
    "class CircleDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, image_size=(64, 64), max_circles=5):\n",
    "        self.binary_masks, self.instance_masks = self.create_dataset(num_samples, image_size, max_circles)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.binary_masks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        binary_mask = self.binary_masks[idx]\n",
    "        instance_mask = self.instance_masks[idx]\n",
    "        instance_mask = np.pad(instance_mask, ((0, 0), (0, 0), (0, 5 - instance_mask.shape[2])), constant_values=0)\n",
    "        return torch.tensor(binary_mask, dtype=torch.float32).unsqueeze(0), torch.tensor(instance_mask, dtype=torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_dataset(num_samples=1000, image_size=(64, 64), max_circles=5):\n",
    "        binary_masks = []\n",
    "        instance_masks = []\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            num_circles = random.randint(1, max_circles)\n",
    "            _, binary_mask, _, _ = generate_image_with_circles(image_size, num_circles)\n",
    "            instances = generate_instance_masks(binary_mask)\n",
    "\n",
    "            binary_masks.append(binary_mask)\n",
    "            instance_stack = np.stack(instances, axis=-1) if instances else np.zeros((*image_size, 0))\n",
    "            instance_masks.append(instance_stack)\n",
    "\n",
    "        return np.array(binary_masks), instance_masks\n",
    "\n",
    "def generate_image_with_circles(image_size=(64, 64), num_circles=5):\n",
    "    \"\"\"Generate an image with random circles and return the image and binary mask.\"\"\"\n",
    "    image = np.zeros(image_size, dtype=np.uint8)\n",
    "    binary_mask = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "    for _ in range(num_circles):\n",
    "        radius = random.randint(4, 8)\n",
    "        center = (\n",
    "            random.randint(radius, image_size[0] - radius),\n",
    "            random.randint(radius, image_size[1] - radius),\n",
    "        )\n",
    "        rr, cc = disk(center, radius, shape=image_size)\n",
    "        binary_mask[rr, cc] = 1\n",
    "\n",
    "    return image, binary_mask, [], []\n",
    "\n",
    "def generate_instance_masks(binary_mask):\n",
    "    \"\"\"Generate instance masks from a binary mask.\"\"\"\n",
    "    labeled_mask = label(binary_mask)\n",
    "    instance_masks = []\n",
    "    for region in regionprops(labeled_mask):\n",
    "        instance_mask = labeled_mask == region.label\n",
    "        instance_masks.append(instance_mask)\n",
    "    return instance_masks\n",
    "\n",
    "\n",
    "def get_cell_count(image): \n",
    "    \n",
    "    # Masked Image (256 x 256) \n",
    "    unique_colors = np.unique(image)\n",
    "    cell_colors = unique_colors[unique_colors != 0]  # Exclude background (color 0)\n",
    "    \n",
    "    # Cell Count\n",
    "    cell_count = len(cell_colors)    \n",
    "    \n",
    "    return cell_count, cell_colors\n",
    "\n",
    "\n",
    "\n",
    "def get_instance_masks(image, plot = None): # Plot Masks - plot = 1\n",
    "    Masks = []\n",
    "    \n",
    "    cell_count, cell_colors = get_cell_count(image)\n",
    "    \n",
    "    #print(cell_count)\n",
    "    \n",
    "    # Create mask for each Cell: \n",
    "    for cell in range(cell_count):\n",
    "\n",
    "        # Cell Color\n",
    "        color = cell_colors[cell]\n",
    "        \n",
    "        if color != 0:\n",
    "            # Find Pixels\n",
    "            cell_indices = (image.flatten() == color)\n",
    "\n",
    "            \n",
    "            # Form mask of cell in a 256x256 image with black background (all other pixels in image are colored 0)\n",
    "            mask = np.zeros((image.shape[0]**2, 1), dtype=np.uint8)        \n",
    "            mask[cell_indices] = color\n",
    "\n",
    "            mask = mask.reshape((image.shape[0], image.shape[0]))\n",
    "            \n",
    "            if plot == 1:\n",
    "                # Visualize the mask\n",
    "                plt.figure()\n",
    "                plt.imshow(mask, cmap=\"gray\")\n",
    "                plt.title(f\"Mask for Cell {cell + 1}\")\n",
    "                plt.axis(\"off\")\n",
    "            Masks.append(mask)\n",
    "            \n",
    "    #Found max cell count to be 109\n",
    "    #This will only work on same size images\n",
    "    if len(Masks) < 109:\n",
    "        for k in range(109-len(Masks)):\n",
    "            mask = np.zeros((image.shape[0]**2, 1), dtype=np.uint8)  \n",
    "            mask = mask.reshape((image.shape[0], image.shape[0]))\n",
    "            Masks.append(mask)\n",
    "            \n",
    "    if plot == 1:\n",
    "        plt.show()\n",
    "    \n",
    "    return Masks\n",
    "\n",
    "def visualize_results(image, binary_mask, instance_masks):\n",
    "    \"\"\"Visualize the image, binary mask, and instance masks.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2 + len(instance_masks), figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(binary_mask, cmap='gray')\n",
    "    axes[1].set_title(\"Binary Mask\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    for i, instance_mask in enumerate(instance_masks):\n",
    "        axes[2 + i].imshow(instance_mask, cmap='gray')\n",
    "        axes[2 + i].set_title(f\"Instance Mask {i + 1}\")\n",
    "        axes[2 + i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, output_channels=64):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # First Conv Layer\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Downsample by 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Second Conv Layer\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Downsample by 2\n",
    "            nn.Conv2d(64, output_channels, kernel_size=3, stride=1, padding=1),  # Final Conv Layer\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "    \n",
    "class TransformerSegmentationModel(nn.Module):\n",
    "    def __init__(self, cnn_extractor= CNNFeatureExtractor(), img_size=64, patch_size=4, in_channels=1, out_channels=5):\n",
    "        super(TransformerSegmentationModel, self).__init__()\n",
    "        \n",
    "        # Initialize CNN Backbone\n",
    "        self.cnn_backbone = cnn_extractor\n",
    "\n",
    "        # Transformer parameters\n",
    "        embed_dim = 256\n",
    "        num_heads = 8\n",
    "        num_layers = 4\n",
    "        \n",
    "        # Calculate number of patches after CNN\n",
    "        num_patches = (img_size // 4) * (img_size // 4)  # Adjust based on pooling\n",
    "\n",
    "        # Transformer setup\n",
    "        self.flatten_dim = (img_size // 4) * (img_size // 4) * 256  # Adjust for output channels of CNN\n",
    "        self.embedding = nn.Linear(self.flatten_dim, embed_dim)\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Output projection layer\n",
    "        self.output_proj = nn.Conv2d(embed_dim, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Pass through CNN Backbone\n",
    "        x = self.cnn_backbone(x)  # Shape: (batch_size, out_channels, H', W')\n",
    "\n",
    "        # Flatten and prepare for transformer\n",
    "        x = x.flatten(2).permute(2, 0, 1)  # Shape: (num_patches, batch_size, embed_dim)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x += self.position_embedding\n",
    "\n",
    "        # Transformer processing\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Reshape back to image dimensions\n",
    "        x = x.permute(1, 2, 0).view(batch_size, -1, img_size // 4, img_size // 4)  # Adjust based on pooling\n",
    "\n",
    "        # Output projection\n",
    "        x = self.output_proj(x)  # Shape: (batch_size, out_channels, H'', W'')\n",
    "\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TransformerSegmentationModel(nn.Module):\n",
    "    def __init__(self, img_size=256, patch_size=4, in_channels=1, out_channels=109, embed_dim=256, num_heads=8, num_layers=4):\n",
    "        super(TransformerSegmentationModel, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # CNN Feature Extractor\n",
    "        self.cnn_feature_extractor = CNNFeatureExtractor(output_channels=64)\n",
    "        \n",
    "        # Calculate new dimensions after CNN feature extraction\n",
    "        self.feature_size = img_size // 4  # Due to two max pooling operations\n",
    "        self.num_patches = (self.feature_size // patch_size) ** 2\n",
    "        self.flatten_dim = patch_size * patch_size * 64  # 64 is the output channels of CNN\n",
    "\n",
    "        self.embedding = nn.Linear(self.flatten_dim, embed_dim)\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches, embed_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.output_proj = nn.Linear(embed_dim, patch_size * patch_size * out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # CNN Feature Extraction\n",
    "        x = self.cnn_feature_extractor(x)  # Output: [8, 64, 64, 64]\n",
    "        \n",
    "        # Divide features into patches\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        x = x.view(batch_size, self.num_patches, -1)\n",
    "        \n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(x) + self.position_embedding\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.output_proj(x)\n",
    "        \n",
    "        # Reshape into image dimensions\n",
    "        x = x.view(batch_size, self.feature_size // self.patch_size, self.feature_size // self.patch_size, \n",
    "                   self.patch_size, self.patch_size, self.out_channels)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        x = x.view(batch_size, self.feature_size, self.feature_size, self.out_channels)\n",
    "        \n",
    "        # Upsample to original image size\n",
    "        x = nn.functional.interpolate(x.permute(0, 3, 1, 2), size=(self.img_size, self.img_size), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return torch.sigmoid(x.permute(0, 1, 2, 3))\n",
    "\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "# Define a downsampling factor (e.g., reduce to 128x128)\n",
    "#downsample_size = (64, 64)\n",
    "\n",
    "# Function to load, store original, and downsample TIFF images\n",
    "def load_and_downsample_images(folder_path, downsample_size, threshold=None):\n",
    "    tiff_files = [f for f in os.listdir(folder_path) if f.endswith('.tif') or f.endswith('.tiff')]\n",
    "    original_images = []  # Store original images\n",
    "    downsampled_images = []  # Store downsampled images\n",
    "\n",
    "    for file_name in tiff_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        image = Image.open(file_path)#.convert(\"L\")  # Convert to grayscale\n",
    "        original_images.append(np.array(image))  # Store the original image\n",
    "\n",
    "\n",
    "    \n",
    "        # Apply threshold if provided (for masks)\n",
    "        if threshold is not None:\n",
    "            #image = np.array(image)\n",
    "            # Use nearest-neighbor interpolation\n",
    "            #downsampled_mask = Image.fromarray(image).resize((downsample_size), Image.NEAREST)\n",
    "            # Resize to the downsample size\n",
    "            resized_image = image.resize(downsample_size)\n",
    "            resized_array = np.array(resized_image)\n",
    "            #sigma = 1.0  # Adjust based on noise level\n",
    "            #smoothed_image = gaussian_filter(image, sigma=sigma)\n",
    "            #downsampled_image = Image.fromarray(smoothed_image).resize(downsample_size)\n",
    "            \n",
    "            \n",
    "            resized_array = (resized_array > threshold).astype(np.uint8)  # Binary mask\n",
    "            downsampled_images.append(resized_array)\n",
    "        else:\n",
    "            \n",
    "            sigma = 3.0  # Adjust based on noise level\n",
    "            smoothed_image = gaussian_filter(image, sigma=sigma)\n",
    "            downsampled_image = Image.fromarray(smoothed_image).resize(downsample_size)\n",
    "            downsampled_images.append(downsampled_image)\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "    return np.stack(original_images), np.stack(downsampled_images)  # Return both original and downsampled\n",
    "\n",
    "\n",
    "\n",
    "downsample_size = (64,64)\n",
    "\n",
    "\n",
    "# Load original and downsampled brightfield images\n",
    "brightfield_folder = '../../../projectnb/ec523kb/projects/teams_Fall_2024/Team_2/bacteria_counting/Data/2b/DeepBacs_Data_Segmentation_Staph_Aureus_dataset/brightfield_dataset/train/patches/brightfield'\n",
    "original_brightfield, downsampled_brightfield = load_and_downsample_images(brightfield_folder, downsample_size)\n",
    "\n",
    "# Load original and downsampled masks\n",
    "masks_folder = '../../../projectnb/ec523kb/projects/teams_Fall_2024/Team_2/bacteria_counting/Data/2b/DeepBacs_Data_Segmentation_Staph_Aureus_dataset/brightfield_dataset/train/patches/masks'\n",
    "original_masks, downsampled_masks = load_and_downsample_images(masks_folder, downsample_size, threshold=1)\n",
    "\n",
    "# Convert downsampled images and masks to tensors\n",
    "X_tensor = torch.tensor(downsampled_brightfield, dtype=torch.float32).unsqueeze(1)  # (batch_size, 1, H, W)\n",
    "Y_tensor = torch.tensor(downsampled_masks, dtype=torch.float32).unsqueeze(1)  # (batch_size, 1, H, W)\n",
    "\n",
    "X2_tensor = torch.tensor(original_brightfield, dtype=torch.float32).unsqueeze(1)  # (batch_size, 1, H, W)\n",
    "\n",
    "Y2 = []\n",
    "\n",
    "for k in range(len(original_masks)):\n",
    "    instance_Y2 = get_instance_masks(original_masks[k])\n",
    "    Y2.append(instance_Y2)\n",
    "    \n",
    "#print(np.array(Y2).shape)\n",
    "\n",
    "\n",
    "Y2_tensor = torch.tensor(Y2, dtype=torch.float32) # (batch_size, 1, H, W)\n",
    "\n",
    "# Check the new shapes\n",
    "print(f\"Original Brightfield shape: {X2_tensor.shape}\")  # (batch_size, original_height, original_width)\n",
    "print(f\"Downsampled Brightfield shape: {Y2_tensor.shape}\")  # (batch_size, 1, downsampled_height, downsampled_width)\n",
    "\n",
    "# Display a few examples\n",
    "num_examples = 1  # Number of examples to show\n",
    "plt.figure(figsize=(12, num_examples * 3))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    # Original brightfield image\n",
    "    plt.subplot(num_examples, 4, i * 4 + 1)\n",
    "    plt.imshow(original_brightfield[i])\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Downsampled brightfield image\n",
    "    plt.subplot(num_examples, 4, i * 4 + 2)\n",
    "    plt.imshow(X_tensor[i, 0].numpy())\n",
    "    plt.title(\"Downsampled Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Original mask\n",
    "    plt.subplot(num_examples, 4, i * 4 + 3)\n",
    "    plt.imshow(original_masks[i])\n",
    "    plt.title(\"Original Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Downsampled mask\n",
    "    plt.subplot(num_examples, 4, i * 4 + 4)\n",
    "    plt.imshow(Y_tensor[i, 0].numpy())\n",
    "    plt.title(\"Downsampled Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X2_tensor, Y2_tensor)  # Pairs of images and masks\n",
    "#dataloader = DataLoader(dataset, batch_size=8, shuffle=True)  # Modify batch size if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = TransformerSegmentationModel().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for binary_mask, instance_masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        binary_mask = binary_mask.to(device)  # Move binary_mask to device\n",
    "        instance_masks = instance_masks.to(device) \n",
    "        \n",
    "        outputs = model(binary_mask)\n",
    "        \n",
    "        #print('outputs')\n",
    "        #print(outputs.shape)\n",
    "        #print(instance_masks.shape)\n",
    "        \n",
    "        #bce_loss  = criterion(outputs, instance_masks)\n",
    "        #dice_loss = DiceLoss()(outputs, instance_masks)\n",
    "        #loss = bce_loss + dice_loss\n",
    "        \n",
    "        #This will take values outside of 0 to 1. If this messes things up then you can just normalize the input masks\n",
    "        base_loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        #base_loss = nn.BCELoss()\n",
    "        custom_loss = CustomLoss(base_loss, alpha=2.0, beta=10.0)\n",
    "\n",
    "        \n",
    "        \n",
    "        #print(outputs.shape)\n",
    "        #print(instance_masks.shape)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = custom_loss(outputs, instance_masks)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Visualizing the binary mask\n",
    "            binary_mask = binary_mask[0].squeeze(0).cpu().numpy()\n",
    "            instance_masks = instance_masks[0].cpu().numpy()\n",
    "            predicted_masks = outputs[0].cpu().numpy()\n",
    "\n",
    "            # Limit the number of masks to display\n",
    "            max_masks = 10  # Maximum number of masks to display\n",
    "            num_instance_masks = min(instance_masks.shape[-1], max_masks)\n",
    "            num_predicted_masks = min(predicted_masks.shape[-1], max_masks)\n",
    "\n",
    "            # Adjust the layout to fit the limited number of masks\n",
    "            fig, axes = plt.subplots(2, max(num_instance_masks, num_predicted_masks) + 1, figsize=(18, 6))\n",
    "\n",
    "            # Binary Mask\n",
    "            axes[0, 0].imshow(binary_mask, cmap='gray')\n",
    "            axes[0, 0].set_title(\"Binary Mask\")\n",
    "            axes[0, 0].axis(\"off\")\n",
    "            \n",
    "            print(instance_masks.shape)\n",
    "\n",
    "            # Ground Truth Masks (limited to max_masks)\n",
    "            for i in range(num_instance_masks):\n",
    "                axes[0, i + 1].imshow(instance_masks[i,...], cmap='gray')\n",
    "                axes[0, i + 1].set_title(f\"GT Mask {i+1}\")\n",
    "                axes[0, i + 1].axis(\"off\")\n",
    "\n",
    "            # Predicted Masks (limited to max_masks)\n",
    "            for i in range(num_predicted_masks):\n",
    "                axes[1, i + 1].imshow(predicted_masks[i,...], cmap='gray')\n",
    "                axes[1, i + 1].set_title(f\"Pred Mask {i+1}\")\n",
    "                axes[1, i + 1].axis(\"off\")\n",
    "\n",
    "            # Sum of Predicted Masks\n",
    "            predicted_sum = np.sum(predicted_masks, axis=-3)\n",
    "            axes[1, 0].imshow(predicted_sum, cmap='gray')\n",
    "            axes[1, 0].set_title(\"Sum of Predicted Masks\")\n",
    "            axes[1, 0].axis(\"off\")\n",
    "\n",
    "            # Layout adjustment\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
